A data warehouse for Tom's personal life with the following cubes

Terminal commands
    From shell history files
Calendar events
    http://thomaslevine.com/schedule
Facebook messages
    Facebook messages that people have sent me (not messages I have sent)
Facebook chat status changes
    When people go online and offline


Here are some other possible cubes or data sources.

* Git commits
* Project pages (``pip install tlevine``)
* Emails, possibly broken out as
  * Notifications from various services
  * Sent emails
  * Mailing lists
  * Travel bookings
  * ...
* Finances (Gnucash)
* Old calendar (Google Calendar)
* Other Google services, mainly for stuff from years ago
* Text messages
* Meetup.com history
* Facebook chat
* Server logs
* Piwik database
* Mutt aliases

Some tables to create to assist in the creation of these tools

* Unique identifiers for people with links to email addresses, phone
    numbers, &c.
* Cities I've been in, people I stayed with


Notes on cubes
====================
Cubes has enough layers to confuse me, and it's not clear to me whether
the documentation is up to date. Instead of defining my models as cubes
model JSON files, I'm using the underlying stuff.

Models are defined with
`model providers <http://pythonhosted.org/cubes/extensions/providers.html>`_.
The mixpanel model provider, in ``cubes/backends/mixpanel/store.py``,
is a decent example. Here's the method where it loads the Mixpanel model.

    def default_metadata(self, metadata=None):
        """Return Mixpanel's default metadata."""

        model = pkgutil.get_data("cubes.backends.mixpanel", "mixpanel_model.json")
        metadata = json.loads(model)

        return metadata

So I want to define an SQLAlchemy model provider that takes an SQLAlchemy
declarative base as input. I think.


Structure of the repository
==============================

``doeund``
    A separate package for assembling a model from sqlalchemy
``warehouse/{main,model,logger}.py``
    Overall configuration of the data warehouse
The rest of ``warehouse``
    Connectors to different data sources

Thing to consider
http://docs.sqlalchemy.org/en/rel_0_9/orm/extensions/associationproxy.html

To do
=======

* On update of a date or time column, make an appropriate entry in the
    date or time fact table if such an entry does not exist.
* On update of any column referencing a label column (unique column that
    is not the primary key in a two-column table), create the reference if
    it does not already exist.
* http://docs.sqlalchemy.org/en/rel_0_8/orm/session.html#unitofwork-cascades
* https://bitbucket.org/zzzeek/sqlalchemy/wiki/UsageRecipes/UniqueObject


Queries that I want to exist
==============================
Import stuff just in case.

    import dada

Produce a cube/table with the fields "language" (R or shell),
"year", "month", "hour", computer (laptop, nsa, &c.), arg0
(``$0``), "arg1" (``$1``) and, finally, "count", where count
is the number of records that fit in the cell specified by
the other fields.

    cubes['command']\
        .set_cut('language', [['R'],['shell']])\
        .roll_up({'time': ['hour'],
                  'date': ['year', 'month'],
                  'computer': None # Use smallest granularity, with all levels.
                  'args': ['arg0', 'arg1']},
                 dada.rollups.count)

Convert Facebook status changes to hourly buckets of time on Facebook.

    cubes['timeonfacebook']\
        .point_cut('current_nick', ['Thomas Levine'])\
        .roll_up({'time': ['hour', 'minute']}, dada.rollups.sum)
